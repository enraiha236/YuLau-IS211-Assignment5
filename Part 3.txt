With one server the simulation produced an average wait time of about 4,999.97 seconds, showing that the system quickly became overwhelmed as new requests kept arriving faster than they could be handled. The single queue grew very long, and each new request waited longer as the backlog increased. This shows how a single server can become a bottleneck when workload exceeds capacity, leading to very high latency across the system.

When additional servers were introduced the average wait time dropped sharply. With three servers the average wait fell to 0.39 seconds, and with five servers it fell further to 0.02 seconds. These results show how distributing requests across multiple servers greatly reduces congestion and latency. The improvement from one to three servers was dramatic, while the improvement from three to five was much smaller, reflecting diminishing returns once the system can keep pace with incoming requests.